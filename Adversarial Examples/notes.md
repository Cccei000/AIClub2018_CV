# 2018/10/23
## 攻击对象：
* 自己实现的一个简单的6层CNN.
* 前三层卷积层分别有8、16、20个filters，大小均为(5,5).  
* 后两层全连接层大小分别为(320,50)和(50,10).
* 数据集为MNIST，图像像素取值范围为[0,255]，训练集大小为60000，batch为100，训练5轮.
* 最终在2000个测试数据上准确率达到99%.

## 对抗方案1：
### 训练：
* 一个比较自然的想法是针对给定图像和目标标签，反复训练一个噪声图像，使之叠加在给定图像上得到的对抗样本传入CNN后，所得预测标签与目标标签的损失最小.
* 可以用反向传播+梯度下降法直接对噪声进行训练，为了控制噪声的大小，可在原先的交叉熵损失函数上增加一个惩罚项.
* 实现过程中发现训练过程十分缓慢，并且直接使用L2范数做惩罚项效果不好并且很难调控，故使用绝对均值做惩罚项，并对损失进行系数修正.
* 为了进一步控制噪声大小，对噪声进行了截断使其保持在一定范围内.
* 最终版本中，使用SGD进行梯度下降，学习率为0.1，动量系数为0.9。交叉熵系数为5000，均值惩罚项系数为100。截断范围为(-15,15).
### 测试：
* 测试效果时，固定取原训练集的前100个数据作为被攻击的数据，对其中的每一个随机分配一个与其真实标签不同的目标标签，用500个iteration生成一个噪声图像。如果CNN给予对抗样本的标签与目标标签相同，则记1次攻击成功。最终给出攻击的成功率和成功噪声图像的平均L2范数.
* 最终的结果：攻击的成功率为58%，成功案例的平均L2范数为391.
* 一个成功案例：真实标签为5，目标标签为9，CNN预测标签为9，噪声L2范数为393。从左到右分别为原始图像、对抗样本、噪声图像：  
![](https://raw.githubusercontent.com/Cei1ing/AIClub2018_CV/master/Adversarial%20Examples/adversarial_v1.JPG)  
### 问题：
1. 首先即使用了GPU，训练一张噪声图像还是挺慢的，根据训练过程中交叉熵的变化情况，至少要500个iteration才能保证效果.
2. 其次成功率很一般，噪声也不足够小，肉眼并不难观测到噪声，而且噪声的分布很散漫，非常影响在图像背景上的观测效果.
3. 训练过程不可控且不可解释：为什么L2惩罚项不好用？截断范围为怎么选取？交叉熵损失和惩罚项之间的系数怎么选取？都是乱试出来的.

## 对抗方案2：
### 训练：
* 采用FGSM的方法直接训练对抗样本，训练公式为x_n+1 = x_n - lr\*sign(x_n.grad)，并控制对抗样本在原始图像加减eps范围内，且整体在[0,255]内.
* 最终版本中，使用交叉熵作为损失函数，没有修正系数，没有惩罚项，lr取1，eps取25.
### 测试：
* 测试方式与先前相同，最终的攻击成功率为50%，成功案例的平均L2范数为477.
* 一个成功案例：真实标签为7，目标标签为2，CNN预测标签为2，噪声L2范数为479。从左到右分别为原始图像、对抗样本、噪声图像：  
![](https://raw.githubusercontent.com/Cei1ing/AIClub2018_CV/master/Adversarial%20Examples/adversarial_v2.JPG)  
### 问题：
1. 同方案1的问题.
2. lr和eps也很难选取，尤其是eps的效果很难把控，最终对抗样本的像素值几乎都是原始图像像素加减eps.

## 总结：
1. 进一步学习并熟悉了pytorch的各种特性和功能.
2. 实践了一次从分析问题、提出方法、代码实现到细节优化与再生产的过程.
3. 最终的对抗效果亟需提升，虽然参考了网上一些效果惊艳的FGSM代码进行修改，包括进行图像归一化和根据梯度调整步长，但是都不起作用，原因有待进一步研究.

# 2018/10/24
## 攻击对象：
* 对原先的CNN做了一些非本质性的调整.
1. 将输入从[0,255]归一化到[0,1]上.
2. 调整了CUDA内存的使用，减少开销提升速度.
3. 训练10轮，最终用整个测试集的10000张图像进行测试，准确率为98.84%，由于任务重点不在这，故不继续提升.

## 对抗方案1+：
* 思路和框架和原先的对抗方案1一致，但是由于对输入进行了归一化，故对实现和效果进行修改：
1. 改用L2做惩罚项.
2. 不对损失做系数修正.
3. 不使用截断.
* 采取上述修改的原因在于：
1. 如果不使用截断，无论怎么调整绝对均值惩罚项的系数，对抗样本都是糊的，噪声的L2达到20.
2. 如果使用截断，在截断值取0.2的情况下，虽然L2被控制到了5左右，攻击成功率达到100%，但是噪声图像非常散漫，对抗样本其实面目全非了。在使用更小的截断值的情况下，攻击成功率会发生显著衰减.
3. 上述观察至少说明绝对均值做惩罚项是达不到预期目标的了，索性用L2做惩罚项，结果效果极佳。并且使用截断对成功率和L2的影响微弱，但使得噪声散漫，易于观测。而对L2损失项进行系数修正只会对L2和成功率有不显著或更差的效果.
* 最终在1000张图像上测试的攻击成功率达到99%，成功案例的平均L2范数为2.280，为了达到这个效果每个样本的训练只需100个iteration.
* 一个成功案例：真实标签为6，目标标签为8，CNN预测标签为8，噪声L2范数为1.01，相当于256灰度级下的257.6。从左到右分别为原始图像、对抗样本、噪声图像，可以看出噪声似乎“有意图地”把6变成8，这是符合预期的：
![](https://raw.githubusercontent.com/Cei1ing/AIClub2018_CV/master/Adversarial%20Examples/adversarial_v3.jpg)  

### 问题：
1. 如何解释归一化的效果和截断的损害？
2. 失败的样例发生了什么？
3. 为什么有些配置下成功率很高，L2也挺小，但噪声却很散漫？
2. FGSM的效果并没有得到改善.

